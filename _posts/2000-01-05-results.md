---
title: "Results"
bg: #9AD1F5
color: black
style: center
fa-icon: line-chart
---

## Horse2Zebra

Before approaching the faces to caricatures challenge, we had to make sure that we were able to run and train the [Jun-Yan Zhu CycleGan](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix).
First, we tested the network using the author's pretrained model horse2zebraA. Then we trained our own model for only 2 epochs. These experiments were performed to get to know the environment in terms of configuration, execution time and limitations, and the code of the CycleGAN.

<table style="width: 100%">
  <tr>
    <th><img src="./img/training/pretrained_n02381460_1010_real_A.png" alt="Real image"/>
	</th>
    <th><img src="./img/training/pretrained_n02381460_1010_fake_B.png" alt="Result with pretrained Horse2zebra model"/></th> 
    <th><img src="./img/training/trained_2e_n02381460_1010_fake_B.png" alt="Result with model trained for 2 epochs"/></th>
  </tr>
  <tr>
  <th><i>Real image</i></th>
    <th><i>Result generated with pretrained Horse2zebra model</i></th> 
    <th><i>Result with model trained for 2 epochs</i></th>
  </tr>
</table>
<br />
<br />

## Faces to caricatures

In this experiment we aim to train the cycleGAN model for converting from faces to caricatures and viceversa. For the moment we have 3 results: 5 epochs with resizing to 64x64, 5 epochs with 128x128 resizing and 16 epochs with 128x128 resizing.

#### Exp2.1: 5 epochs with 64x64

| ![](./img/training/64x64/64x_006002_real_A.png ) | ![](./img/training/64x64/64x_006007_real_A.png ) | ![](./img/training/64x64/64x_006085_real_A.png) |   
| 	![](./img/training/64x64/64x_006002_fake_B.png ) | 	![](./img/training/64x64/64x_006007_fake_B.png) | ![](./img/training/64x64/64x_006085_fake_B.png ) |


This trained model is not very useful, obviously as the training is very small. It was meant to test for improvement on the training speed.


#### Exp2.2: 5 epochs with 128x128

| ![](./img/training/128x128_5e/006002_real_A.png ) | ![](./img/training/128x128_5e/006007_real_A.png ) | ![](./img/training/128x128_5e/006085_real_A.png ) | ![](./img/training/128x128_5e/006100_real_A.png ) |
|  ![](./img/training/128x128_5e/006002_fake_B.png )  |  ![](./img/training/128x128_5e/006007_fake_B.png ) | ![](./img/training/128x128_5e/006085_fake_B.png ) | ![](./img/training/128x128_5e/006100_fake_B.png ) |

We get better results than with 64x64, we suspect the networks are not suitable for 64x64 dimension. Still those are not good results.


#### Exp2.3: 16 epochs with 128x128

| ![](./img/training/128x128_16e/006002_real_A.png ) | ![](./img/training/128x128_16e/006007_real_A.png ) | ![](./img/training/128x128_16e/006085_real_A.png ) | ![](./img/training/128x128_16e/006100_real_A.png ) |
|  ![](./img/training/128x128_16e/006002_fake_B.png )  |  ![](./img/training/128x128_16e/006007_fake_B.png ) | ![](./img/training/128x128_16e/006085_fake_B.png ) | ![](./img/training/128x128_16e/006100_fake_B.png ) |

At this point the output seems to be worse than in the beginning.

<p class="caption"> <img src="./img/training/128x128_16e/local_14e_128x128_sample2.png"><br />Output from Generator A</p>

The Generative B network starts to show better results, up until now it was absolutely unusable, we start to see some more realistic faces generated from caricatures but they are still far from being close to real.

<p class="caption"> <img src="./img/training/128x128_16e/local_13e_128x128_sample9.png"><br />Output from Generator B</p>

But shape transformation still being imperceptible.
We have trained for more than 16 epochs and the results are not better.
<p class="caption"> <img src="./img/training/128x128_16e/local_15e_128x128_sample9.png"><br />Output after 16 epochs training</p>

In conclusion, we are not getting generative network A to caricaturize at all, it transforms a portrait into a drawing.



#### Exp2.4: 100 epochs with 64x64

| ![](./img/training/64x64_100e/006002_real_A.png ) | ![](./img/training/64x64_100e/006007_real_A.png ) | ![](./img/training/64x64_100e/006085_real_A.png ) | ![](./img/training/64x64_100e/006100_real_A.png ) |
|  ![](./img/training/64x64_100e/006002_fake_B.png )  |  ![](./img/training/64x64_100e/006007_fake_B.png ) | ![](./img/training/64x64_100e/006085_fake_B.png ) | ![](./img/training/64x64_100e/006100_fake_B.png ) |

Results are now stable, all generated images appear to be drawings from portraits, some in color others in black & white. There are no longer strange results. The training is 6 times faster (4 minutesper each epoch) but the generative model is not modifying the shape that a caricature is expected to do.

<br />
## Mini GAN
We have trained the GAN model in Google Cloud with a Tesla V100 GPU for 50 epochs, and the results are the following ones:

<table style="width:100%">
  <tr>
    <th><img src="./img/GAN_epoch001.png" alt="Epoch001"/></th>
    <th><img src="./img/GAN_epoch050.png" alt="Epoch050"/></th> 
    <th><img src="./img/GAN_generate_animation.gif" alt="Training gif"/></th>
  </tr>
  <tr>
    <th class="caption">GAN epoch 01</th>
    <th class="caption">GAN epoch 50</th> 
    <th class="caption">GAN training gif</th>
  </tr>
</table>

<br />

As expected, the GAN is able to generate the MNIST samples. It is
possible to see how the readability of the numbers increases along the
epochs, and possibly, it could be better with more training epochs.

<br />
## Mini CycleGAN
We have trained the CycleGAN model in Google Cloud with a Tesla V100 GPU for 100 epochs, and the results are the following ones:

<table style="width:100%">
  <tr>
     <th><img src="./mini-cyclegan-results-images/train-samples-A2B/46_1_input.png" alt="Input"/></th>
     <th><img src="./mini-cyclegan-results-images/train-samples-A2B/46_1_output.png" alt="Output"/></th>
     <th><img src="./mini-cyclegan-results-images/train-samples-A2B/46_1_recon.png" alt="Reconstruction"/></th>
  </tr>
  <tr>
     <th class="caption">Horse train sample input</th>
     <th class="caption">Generated zebra</th>
     <th class="caption">Horse reconstruction</th>
  </tr>
</table>

<br />

<table style="width:100%">
  <tr>
    <th><img src="./mini-cyclegan-results-images/train-samples-B2A/48_7_input.png" alt="Input"/></th>
    <th><img src="./mini-cyclegan-results-images/train-samples-B2A/48_7_output.png" alt="Output"/></th>
    <th><img src="./mini-cyclegan-results-images/train-samples-B2A/48_7_recon.png" alt="Reconstruction"/></th>
  </tr>
  <tr>
    <th class="caption">Zebra train sample input</th>
    <th class="caption">Generated horse</th>
    <th class="caption">Zebra reconstruction</th>
  </tr>
</table>

<br />

<table style="width:100%">
  <tr>
    <th><img src="./mini-cyclegan-results-images/test-samples-A2B/49_110_input.png" alt="Input"/></th>
    <th><img src="./mini-cyclegan-results-images/test-samples-A2B/49_110_output.png" alt="Output"/></th>
    <th><img src="./mini-cyclegan-results-images/test-samples-A2B/49_110_recon.png" alt="Reconstruction"/></th>
  </tr>
  <tr>
    <th class="caption">Horse test sample input</th>
    <th class="caption">Generated zebra</th>
    <th class="caption">Horse reconstruction</th>
  </tr>
</table>

<br />

<table style="width:100%">
  <tr>
    <th><img src="./mini-cyclegan-results-images/test-samples-B2A/49_130_input.png" alt="Input"/></th>
    <th><img src="./mini-cyclegan-results-images/test-samples-B2A/49_130_output.png" alt="Output"/></th>
    <th><img src="./mini-cyclegan-results-images/test-samples-B2A/49_130_recon.png" alt="Reconstruction"/></th>
  </tr>
  <tr>
    <th class="caption">Zebra test sample input</th>
    <th class="caption">Generated horse</th>
    <th class="caption">Zebra reconstruction</th>
  </tr>
</table>

<br />

We conclude that the problem and the dataset are too complex to be solved using a tiny network with low capacity.

The authors of the paper train over 200 epochs with a high capacity network, but in our case, around the epoch 80, the generator starts to generate strange images, changing the color of the whole image a lot.
This is why we think that the problem of the system is the network capacity and not the training. 

In both domains, the reconstruction is really close to the input image, so we have the certainty that the cyclic loss is working. 

Here are some of the strange samples generated around the 80 epoch mark:

<table style="width:100%">
  <tr>
    <th><img src="./mini-cyclegan-results-images/strange/101_10_input.png" alt="Input"/></th>
    <th><img src="./mini-cyclegan-results-images/strange/101_10_output.png" alt="Output"/></th>
    <th><img src="./mini-cyclegan-results-images/strange/101_10_recon.png" alt="Reconstruction"/></th>
  </tr>
  <tr>
    <th class="caption">Zebra test sample input</th>
    <th class="caption">Generated horse</th>
    <th class="caption">Zebra reconstruction</th>
  </tr>
</table>

<br />

<table style="width:100%">
  <tr>
    <th><img src="./mini-cyclegan-results-images/strange/101_8_input.png" alt="Input"/></th>
    <th><img src="./mini-cyclegan-results-images/strange/101_8_output.png" alt="Output"/></th>
    <th><img src="./mini-cyclegan-results-images/strange/101_8_recon.png" alt="Reconstruction"/></th>
  </tr>
  <tr>
    <th class="caption">Zebra test sample input</th>
    <th class="caption">Generated horse</th>
    <th class="caption">Zebra reconstruction</th>
  </tr>
</table>
