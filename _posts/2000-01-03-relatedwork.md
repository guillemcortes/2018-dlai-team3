---
title: "Related Work"
bg: #9AD1F5
color: black
fa-icon: puzzle-piece
---

When it comes to image to image translation there have been several implementations using neural networks that preceed cycleGANs.

### [Neural Style Transfer](https://arxiv.org/abs/1508.06576)

![Proposed network](./img/basicstyletransfer.png)

The first implementation of style transfer in the realm of neural networks took a CNN trained for image classification and separated the part of it that learnt texture information. Applying it to a new image. It used a square error loss  to minimize the distance between the 2 feature representations extracted from passing a white noise image and a picture trough the network. 
The result was that the image had a different texture applied but kept the original shape so it had a clear artificial look. CycleGANs on the othe hand learn a mapping between the collections of images rather than between 2 images and this allows for more realistic and flexible transformations. 

![Comparison of method results](./img/neuralstylecomparison.png)

### [Generative Adversarial Networks(GANs)](https://arxiv.org/pdf/1406.2661.pdf)

![](./img/GAN_diagram.jpg)

This solution made a significan change in the success of image to image translatio. It consists of having two models trained simultaneously: G, a generative model which generates images by learning the distribution of information, and D, a discriminative model that is trained to distinguish wether a sample came from the training data rather than G.
This method generates images from random noise and has attained great results in image generation. It succeeds in generating images that look indistinguishable from real images from the generated domain. However, when generating information from an image much of the contextual information is lost and it's imposible to go back to the original data from the generated content.

### [Conditional Generative Adversarial Networks](https://arxiv.org/pdf/1406.2661.pdf)

This solution uses applies a condition to GANs in order to learn a translation between the input and output datasets. This method gives great results but it depends on a great number of pairings of input and output images. CycleGANs build on this idea without overcoming that limitation by pining 2 GANs against each other with an adequate loss function so that the pairs are generated by the competing networks.

### Other methods
Other methods have been presented that rely on predefined similarities between the input and output. CycleGANs overcome this by letting the networks learn the relationship between the domains.

![Comparison with other methods](./img/comparisonmethods.jpg)

### [CycleGANs](https://arxiv.org/abs/1703.10593)

This novel approach appears due to the motivation to translate images to other domains keeping the content of the original image. Until 2017 when -Jun-Yan Zhu et al. released the first CycleGAN- the only way to do it was by having a very specific, paired dataset with the same images in both domains. With the CycleGANs, it is posible to convert an image to another domain and then, reconstruct the original image.
To do so, an easy but smart idea comes to mind. What if we train two GANs together? We have one generator that transforms images from domain A to domain B, another generator that does the opposite and two discriminators, one for each domain.

| ![Simplified view of CycleGAN architecture 1](./img/simplifiedcyclegan_1.png) | ![Simplified view of CycleGAN architecture 2](./img/simplifiedcyclegan_2.png) |

The key of the CycleGANs is the loss used to train an achieve the best performance. The cycle consistency loss -which is detailed in the [Our Work](https://telecombcn-dl.github.io/2018-dlai-team3/#work) section below- takes into account the losses of the generators, the losses of the discriminators, and a cycle loss that evaluates the reconstructed image comparing it with the original one.
