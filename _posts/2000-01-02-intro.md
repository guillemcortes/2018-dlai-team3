---
title: "Introduction"
bg: blue
color: white
fa-icon: quote-left
---

<p style="text-align: justify;">Image to image translation (e.g. style transfer) is a very studied problem in machine learning and many solutions have been proposed. However, all of this solutions rely on having paired data, which many times, it is almost impossible . With the advent of the Generative Adversarial Networks – one the most revolutionary concepts in Deep Learning in the last few years – a new “concept” appears: we can create images with a certain criteria without having paired data. This leaves the question: How we can keep the content (semantic content) of the original photo but with the style of a new one? Here’s where cycleGANs enters the picture. A GANs based network that keep the content of the original image and is capable transforming it to another domain but also reconstructing the original image.
<br><br>
The aim of this project is to understand one of the newest and more revolutionary concepts in deep learning: the GANs. And furthermore, the CycleGANs.
We resume all this process of learning in this page. First we present other works that offer solutions to the style transfer challenge. Then our work is detailed where we emphasize the novelty in the CycleGAN approach: the cycle consistency loss. We also explain the difficulties we have faced adapting the CycleGAN implementation to the picture to caricatures domain. Finally, we present our results and conclusions.</p>

