---
title: "Our Work"
bg: blue
color: white
fa-icon: code-fork
---
# Our mini CycleGAN

In order to fully understand the Generative Adversarial Networks and the CycleGAN once we have studied how they work, we wanted to implement from scratch a very basic examples of this networks, which suppose a proof of concept. In this section, we describe our implementation of a GAN, having the architecture of the [znxlwm](https://github.com/znxlwm/pytorch-generative-model-collections) as reference; and the novel concept of _cycle consistency loss_ introduced by CycleGAN. Another motivation to develop these two light systems was the fact that they are able to easily run and do not require heavy computation power – which is always welcome though. This lightness will obviously affect the performance, but since this is a proof of concept, we don’t aim for the best performance.

The generator and discriminator networks are the following:

<p class="center">    <img src="./img/gen_dis_diagram.png"> </p>

All the used layers except the batch normalization had been already explained during the [DLAI course lessons](https://telecombcn-dl.github.io/2018-dlai/#lectures). The Batch Normalization layer is inspired by S. Ioffe and C. Szegedy in their paper [Accelerating training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)

In a nutshell, the batch normalization layer is a mechanism that eliminates the Internal Covariance Shift – the change in the distributions of internal nodes of a deep network in the course of training – accelerating the training process of deep neural nets. For instance,  it prevents the training from getting stuck in the saturated regimes of nonlinearities. This is why the batch normalization is used before each activation function. Some of the advantages given by the batch normalization are the following:
+ Allows us to use much higher learning rates and be less careful about initialization.
+ In traditional deep networks, too-high learning rate may result in the gradients that explode or vanish, as well as getting stuck in poor local minima. Batch Normalization helps address these issues.
+ By normalizing activations throughout the network, it prevents small changes to the parameters from amplifying into larger and suboptimal changes in activations in gradients; for instance, it prevents the training from getting stuck in the saturated regimes of nonlinearities.
+ It also acts as a regularizer, in some cases eliminating the need for Dropout.
+ Can be applied to any set of activations in the network.
+ The normalization of activations that depends on the mini-batch allows efficient training, but is neither necessary nor desirable during inference; we want the output to depend only on the input, deterministically.

During the implementation of the GAN we have learnt deeper the generator and discriminator networks function and how both networks work together to achieve better results. We have also studied what is a batch normalization layer, how it works and its main purpose, make training faster by eliminating the internal covariance shift. Then, we also have seen that it is a good practise to put more effort on training the discriminator more than the generator, because a better discriminator implies a better generator. Finally we have understood the training process of a GAN.

The training of the GAN commonly starts updating the discriminator. To train the discriminator, we inject real data and compare the output with a ones vector using binary cross-entropy loss criteria. Then we inference with fake data and compare the output with a zero vector using the same criteria. Finally we sum the both losses and update the weights of the discriminator. We do this update once in each step, but it could be done more than once to put more effort on training the discriminator than the generator, as mentioned before. 

Then, to update the generator weights, we generate samples with the generator network, then that samples are discriminated and the output of the discriminator is compared with a ones vector, because we want out generator to generate real samples, using the same criteria as above, the BCE loss. Finally we backpropagate the error and update the weights of the generator network.

Then we have implemented the tiny CycleGAN using the same generator and discriminator networks.

The training of the CycleGAN consists on the same two main stages of a GAN training (update discriminator and generator) but with more complexities, because we have two generators and discriminators.
 
As we train for equal both generators and discriminators, in this case, we start the training step updating the generators. 
First of all, we inject real data into the A2B generator, where A is the input domain and B the output one. Then we discriminate the output of the generator with discriminator B and compute the MSE loss with respect to a ones vector. Next, the output of the A2B generator is used as input for the B2A generator, in order to reconstruct the original image, and the cycle loss is calculated, following the given formula.

<p class="center">
  <img src="./img/firstFormula.png" />
</p>

This process is repeated with the B2A generator, then we sum the four losses and update the weights of both generators. Both generators are updated with the same gradients and adding its losses and backpropagating the aggregate errors, this is because it have no sense to update the generators weights one by one when we want to achieve perfect reconstruction of the images when concatenating the both generators.

<p class="center">
  <img src="./img/secondFormula.png" />
</p>

Once the generators are updated, it’s time to update the A and B domain discriminators. In this case the discriminators are trained one by one, following the same process like in the GAN explained before, but in this case using the MSE loss instead of the BCE one. 


<p class="center">
  <img src="./img/thirdFormula.png" />
</p>

The implementation of the CycleGAN have helped us to fully understand the its behaviour, as well as, how the cyclic loss is able to maintain the content of the images in both A and B domains.


## Replica

Once our understanding of cycleGAN was solid enough, we wanted to experiment with the state-of-the-art related to cycleGANs. We decided to play with the cycleGAN developed by Zhu et al., the code base of the reference paper of this project.


### Setup

We setup the environment locally, on Google Colab and in Google Cloud. For the local setup, we need to setup cuda9.2 and PyTorch with correct nvidia drivers. The local GPU memory size imposed some restrictions on batch and image size. The advantage of training locally is that we can use the View server to see how the network improves or worsens during subsequent training epochs. The implementation by Zhu et al. includes an http server (the so called View server) that shows the evolution of the different losses as well as a sample of how images from both domains are transformed and recovered back. For Google Colab environment, some tricky adjustment were made, deactivate View server, and modify some parts of the PIL module due to compatibility with already installed Python modules. We also transformed the code into a notebook by copying parts of the train.py and test.py into the notebook to modify it properly (for example by passing the command line options that control how the model is trained). For the Google Cloud environment, the setup was similar to the local one, except that for the View server to be available we had to open an http port in the firewall(security group) of the virtual machine.


### Testing a pretrained model and training it from scratch

Our first experiment is to use one of the pre-trained models from the reference paper and run it. The code from CycleGAN paper has 2 scripts for downloading datasets and pre-trained models. We downloaded the Horse2Zebra dataset and their corresponding pre-trained model. Only the Generative network that transforms images of horses into zebras is available. Many options are available in the code base to configure the script test.py and how the model is tested and the output is handled. We can setup how many pictures from the testing folder will be used to perform the test. The dataset folder must contain the folders: trainA, trainB, testA, testB. There's an option to indicate which network we want to test, whether it's GA (from dataset A to B ) or GB (from dataset B to A ).

We also trained from the scratch the horse2zebra model by few epochs. We use the available train.py script and  setup all the command line options. We setup a 2 epochs training, with saving every 1 epoch, showing results every 250 samples trained with an image size of 128x128 setting up the resize and crop option. We deactivate all calls to the view server when executing this training in Colab and the training takes around 1700 seconds per epoch.




### Caricatures datasets

Our second experiment is to try to train a CycleGan to generate images from a human faces dataset to a caricature face dataset. What datasets are available? We start with the celebrity faces dataset and a caricature dataset from Kaggle https://www.kaggle.com/ranjeetapegu/caricature-image. Both datasets are not of the same size, so we extract 10000 celebrity face pictures and distribute 5000 to each trainA and testA folder. Then we distribute both train and test pictures from caricatures dataset to trainB and testB. We observe that the caricatures dataset in use is not very homogeneous, has many caricatures showing parts of the body (not only the face) and that it's limited to 8 celebrities only,(with many different caricatures for each celebrity). We set the training script to resize the images to 128x128, and to use 4 or 8 images per batch (depending on 4GB gpu or 8GB GPU). We try to increment the batch size while there's some GPU memory still left. 

As presented in the results section the model takes a long time to train and does not show signs of improvement.
We study the following improvement ideas:

1. Resizing: adapt network to 64x64 to be able to train faster. We guess the networks are not suitable for 64x64 images since the initial results where much worse than with 128x128 image sizes.

2. Grey scale images: another suggestion is to convert the images to grey scale to get the networks to focus on contours an forms and forget about colors. We estimate this would have a better result visually.

3. Transfer learning (fine-tuning): download a pretrained model (any could do work, but we suggest one that works with datasets that are close to hours). We would retrain again the whole model for a few epochs and see the results.

4. Transfer learning (retraining last layers): download a pretrained model also but instead of training all of it, just update the weights on some of the last layers and freeze the rest. We guess that for this approach, we should update weights at least in all of the 4 networks (GA,DA,GB,DB) or even worse, since discriminator network must be very good to make generator network improve, we would maybe be forced to retrain all discriminator weights and just a few layers of the generator ones.

The improvements 3 and 4 are not feasible, because in one hand the models available from the creators of cycleGAN paper are not related to faces. In the other hand they only offer to download the Generative model network A, we would lack the discriminative network A and both Generative and discriminative networks for B dataset. For the second improvement, it turned out that with 64x64 size style obtained covers both color and contours with good results, so it is not needed.

#### Adapting the network to work with 64x64 images

First we verify that changing the parameter of the size of the image is enough for the generator and discriminator networks to work with it. We verify that the networks are trained from scratch in the normal training setup. Resnet, PatchGAN, etc.. All their structure is defined within the code in a way that builds the network from 0, using the native functions of torch.nn (like nn.Conv2d() ) which initializes the weigths to random values. We can then safely modify parameters and retrain them. We run the training for some epochs with images resized to 64x64 and we observe the output is blurry.

<p class="center">
   <img src="./img/training/64x64_100e/test0.png">
</p>

As the image is smaller, multiple layers of convolution are also performing a pooling or downsampling effect. We decide to try to remove pooling layers and reduce the number of convolutions.  Inspecting the generative and discriminative models (Resnet and NLayerDiscriminator from PatchGAN) we see almost no pooling layers but we see several parameters that could have an impact on the blurry effect, which are:

* the number of ResNetBlocks in Generative network (default is 9)
* the number of downsampling layers in ResNet (Generative network), consisting of Convolution, normalization and Relu (default value is 2)
* the number of discriminator layers in Discriminative network (default value is 3)
* a parameter that affects the number of layer's outputs of the discriminative networks (minimum discriminator layer output default value is 8)
* the parameters ndf and ngf that affect the number of layers outputs in both generative and discriminative networks (default value is 128 and 128)


Several results are shown below:

<p class="center">    <img src="./img/training/64x64_100e/test1.png">
</p>
<p class="center">
	6 blocks ResNet and 2 layers Discriminator	 
</p>
<p class="center">    <img src="./img/training/64x64_100e/test2.png">
</p>
<p class="center">
 ndf/ngf=32, 6 blocks ResNet, downsampling param in ResNet as 1 and 3 layer Discriminator  	

</p>
<p class="center">    <img src="./img/training/64x64_100e/test3.png">
</p>
<p class="center">6 blocks ResNet, downsampling param in ResNet as 1, 1 layer Discriminator and minimum discriminator layer output = 2 

</p>


We then select the last setup to train our 64x64 image size model (See results section). The results is that this model is not generating shape transformations from faces to caricatures. For the other aspects, after 100 epochs, the generated pictures from faces appear to be drawings but closer than previous results. We also gained a speed up of 6, now an epoch takes 4 minutes in Colab, versus the more than 30 min it took before.



#### CariGAN paper approach

Our best model does not still generate geometric transformations of the image of a face to turn it into a caricature. The research performed by [CariGANs: Unpaired Photo-to-Caricature Translation]  CAO et. al, allows to implement a double cycleGAN able to transfer style and geometric transformation to generate caricatures from images o faces.

<p class="center">    <img src="./img/cariGAN.png"> </p> 

The first cycleGAN is similar to the one replicated in this work, but with some improvements to allow for more diversity in styles. The second cycleGAN is in charge of the geometric transformations. This second cycleGAN uses face picture landmarks as inputs and outputs. It trains two models A and B that generate landmarks of caricatures from landmarks of faces and vice-versa. One important technique they used to be successful is to use the Principal Components from s PCA of the landmarks, in order to capture the most significant geometrical characteristics of faces. In other words, they obtained the combination of landmarks that are most representative of the difference between images of faces and used that as an input and output to their model. Another important technique in this cycleGAN, and in the same direction than previous one,is a characteristic loss to enforce exaggerations of distinct facial features only, and avoid arbitrary distortions.

Finally, both cycleGAN models are combined at generation time by warping a generated stylized image into a caricature by the generated landmark.

Note: in that specific example landmarks are 2D points that indicate the location of specific face components like eyes, chin, mouth, nose, etc..  Usually experts define anatomical points to ensure their correspondences within the same species
Note2: Image warping is the process of digitally manipulating an image such that any shapes portrayed in the image have been significantly distorted.



