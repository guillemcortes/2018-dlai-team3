---
title: "Introduction"
bg: blue
color: white
fa-icon: quote-left
---

Image to image translation (e.g. style transfer) is a very studied problem in machine learning and many solutions have been proposed. However, all of this solutions rely on having paired data, which many times, it is almost impossible. With the advent of the Generative Adversarial Networks – one the most revolutionary concepts in Deep Learning in the last few years – a new approach appears: we can create images with a certain criteria without having paired data. This leaves the question: How we can keep the content (semantic content) of the original photo but with the style of a new one? Here’s where [CycleGAN](https://arxiv.org/abs/1703.10593) enters the picture. A GAN based network that keep the content of the original image and is capable of transforming it into another domain but also reconstructing the original image. And all of this, without requiring paired training data.

The aim of this project is to understand one of the newest and more revolutionary concepts in deep learning: the GANs. And furthermore, the CycleGAN. We resume all this process of learning in this page. First we present other works that offer solutions to the style transfer challenge. Then our work is detailed where we emphasize the novelty in the CycleGAN approach: the _cycle consistency loss_. We also explain the difficulties we have faced adapting the CycleGAN implementation to another challenge: pictures to caricatures. Finally, we present our results and conclusions.

