---
title: "Our Work"
bg: blue
color: white
fa-icon: code-fork
---


## Replica

### Google Colab setup

To prepare the envirohment, we need to install PyTorch and some dependencies (We install torch-0.4.1, torchvision and PIllow-5.0.0). The need to register some modification to the PIL module.
We then do a git clone from our modification of the CycleGAN code (we fix a small problem with the way PyTorch handles gpu ids, in some parts it expects strings and in others it expects integers).
Then we can download the datasets to be saved in the datasets folder. We download the horse2zebra dataset.
To run the training or testing part of the CycleGAN we do a small trick that is copying train.py in the notebook, and programatically creating the object opt which will hold the command line and default options  (batch_size, picture dimensions, number of epochs, training continuation,etc)
We also need to deactivate the calls to the View server, since we have no way to publish a port from a Colab vm.


### Local setup

For the local setup, we need to setup cuda9.2 and PyTorch with correct nvidia drivers. Our local GPU is only 4 GB memory so we need to limit the batch size of the model when doing the training. We can adjust the batch_size by a command line option, and also by resizing the images to smaller resolution (128x128 or 64x64).
THe advantage of training locally is that we can use the VIew server to see how the network improves or worsens.


### Google Cloud Platform setup

We haven't tried to train our model in this platform yet.



### Testing pretrained model

Our first experiment is to use a pretrained model from the researches that published the CycleGAN paper and test its working for some of the datasets published by the same authors.

#### checkpoints and results folders
The testing calls models that have their weights saved in the checkpoints folder with the name of the model as a folder name. We create this structure to download there the horse2zebra model

#### dataset & pretrained model download
The code from CycleGAN paper has 2 scripts for downloading datasets and pretrained models. We download both the horse2zebra dataset and the pretrained horse2zebra model.

The results of the test (real and generated images) are saved to the results folder (which we create).

#### command line options
We copy the test.py code into the notebook and we programatically generate the opt object which holds the command and default options for running the training. The model name needs to be specified(corresponds to the folder that holds the network weights inside checkpoints). We can also setup how many pictures from the testing folder will be used to perform the test. The dataset folder must contain the folders: trainA, trainB, testA, testB. There's an option to indicate which network we want to test, whether it's GA (from dataset A to B ) or GB (from dataset B to A )

### Training and testing a model


Our second experiment is to train the horse2zebra model by few epochs.
We download the dataset as explained before, but we remove the checkpoints folder.
We copy the train.py code into the notebook and setup an object opt that will hold all the command line options. We setup a 2 epochs trainig, with saving every 1 epoch, showing results every 250 samples trained with an image size of 128x128 seting up the resize and crop option. We deactivate all calls to the view server when executing this training in Colab. The training takes aroun 1700seconds per epoch.

Taking a look in the code, we observe that the networks used are 9 block Resnet for generative networks and PatchGAN for Discriminative networks.


### Caricatures datasets

Our third experiment is to try to train a CycleGan to generate images from a human faces dataset to a cariacture face dataset. 

What datasets are available? we start with the celebrity faces dataset and a caricature dataset from Kaggle https://www.kaggle.com/ranjeetapegu/caricature-image. Both datasets are not of the same size, so we extract 10000 celebrity face pictures and distribute 5000 to each trainA and testA folder. Then we distribute both train and test pictures from caricatures dataset to trainB and testB. We observe that the caricatures dataset in use is not very homogeneous, has many caricatures showing parts of the body (not only the face) and that is limited to 8 celebrities only,(with many different caricatures for each celebrity). 

Did we setup any preprocessing? We set the training scrip to resize the images to 128x128, and to use 4 or 8 images per batch (depending on 4GB gpu or 8GB GPU). We try to  increment batchsize while GPU memory does not break.


We suggest to try the following improvements:

1. adapt network to 64x64 to be able to train faster. We guess the networks are not suitable for 64x64 images since the initial results where much worse than with 128x128 image sizes.

2. black&white another suggestion is to convert the images to black&white to get the networks to focus on contours an forms and forget about colors. We estimate this would have a better result visually.

3. transfer learning (fine-tuning): download a pretrained model (any could do work, but we suggest one that works with datasets that are close to hours). We would retrain again the whole model for a few epochs and see the results.

4. transfer learning (retraining last layers): download a pretrained model also but instead of training all of it, just update the weights on some of the last layers and freeze the rest. We guess that for this approach, we should update weights at least in all of the 4 networks (GA,DA,GB,DB) or even wors, since discriminator network must be very good to make generator network improve, we would maybe be forced to retrain all discriminator weights and just a few layers of the generator ones.

#### Adapting the network to work with 64x64 images

First we verify that changing the parameter of the size of the image is enough for the generator and discriminator networks to work with it. We verify that the networks are trained from scratch in the normal training setup. Resnet, PatchGAN, etc.. all their structure is defined within the code in a way that builds the network from 0, using the native functions of torch.nn (like nn.Conv2d() ) which initializes the weigths to random values. We can then safely modify parameters and retrain them. We run the training for some epochs with images resized to 64x64 and we observe the output is blurry.

![](./img/training/64x64_100e/test0.png )

As the image is smaller, multiple layers of convolution are also performing the a pooling or downsampling. We decide to try to remove pooling layers and reduce the number of convolutions.  Inspecting the generative and discriminative models (Resnet and NLayerDiscriminator from PatchGAN) we see several parameters that could have an impact on the blurry effect, which are:

* the number of ResNetBlocks in Generative network (default is 9)
* the number of downsampling layers in ResNet, consisting of Convolution, normalization and Relu (default value is 2)
* the number of discriminator layers in Discriminative network (default value is 3)
* a parameter that affects the number of layer's outputs of the discriminative networks (minimum discriminator layer output default value is 8)
* the parameters ndf and ngf that affect the number of layers outputs in both generative and discriminative networks (default value is 128 and 128)


Several results are shown below:

| ![](./img/training/64x64_100e/test1.png ) | 6 blocks ResNet and 2 layers Discriminator	 |
| ![](./img/training/64x64_100e/test2.png ) | ndf/ngf=32, 6 blocks ResNet, downsampling param in ResNet as 1 and 3 layer Discriminator  	 |
| ![](./img/training/64x64_100e/test3.png ) | 	6 blocks ResNet, downsampling param in ResNet as 1, 1 layer Discriminator and minimum discriminator layer output = 2 |

We then select the last setup to train our 64x64 image size model (See results section). The results is that this model is not generating shape transformations from faces to caricatures. For the other aspects, after 100 epochs, the generated pictures from faces appear to be drawings but closer than previous results. We also gained a speed up of 6, now an epochs takes 4 minutes in Colab, versus the more than 30 min it took before.



### Mini-CYCLEGAN
for a better understanding of the cyclegan we decided to implement a tiny/basic
cycleGAN and compare it with its homologue GAN.
For this we decided to work with one of the simplest and widely extended
benchmark dataset: the MNIST. So, our purpose is to generate MNIST numbers 
using a GAN and a cycle GAN.
We implement it ni pytorch and using CPU and GPU computational power.
We development has been carried out in google cloud plattform.


