---
title: "Our Work"
bg: blue
color: white
fa-icon: code-fork
---


## Replica

### Google Colab setup

To prepare the envirohment, we need to install PyTorch and some dependencies (We install torch-0.4.1, torchvision and PIllow-5.0.0). The need to register some modification to the PIL module.
We then do a git clone from our modification of the CycleGAN code (we fix a small problem with the way PyTorch handles gpu ids, in some parts it expects strings and in others it expects integers).
Then we can download the datasets to be saved in the datasets folder. We download the horse2zebra dataset.
To run the training or testing part of the CycleGAN we do a small trick that is copying train.py in the notebook, and programatically creating the object opt which will hold the command line and default options  (batch_size, picture dimensions, number of epochs, training continuation,etc)
We also need to deactivate the calls to the View server, since we have no way to publish a port from a Colab vm.


### Local setup

For the local setup, we need to setup cuda9.2 and PyTorch with correct nvidia drivers. Our local GPU is only 4 GB memory so we need to limit the batch size of the model when doing the training. We can adjust the batch_size by a command line option, and also by resizing the images to smaller resolution (128x128 or 64x64).
THe advantage of training locally is that we can use the VIew server to see how the network improves or worsens.


### Google Cloud Platform setup

We haven't tried to train our model in this platform yet.



### Testing pretrained model

Our first experiment is to use a pretrained model from the researches that published the CycleGAN paper and test its working for some of the datasets published by the same authors.
The notebook can be found here: https://colab.research.google.com/drive/1tYMFpiXBKAoizQqvOZ5odGp9UWBIXuO0

#### checkpoints and results folders
The testing calls models that have their weights saved in the checkpoints folder with the name of the model as a folder name. We create this structure to download there the horse2zebra model

#### dataset & pretrained model download
The code from CycleGAN paper has 2 scripts for downloading datasets and pretrained models. We download both the horse2zebra dataset and the pretrained horse2zebra model.

The results of the test (real and generated images) are saved to the results folder (which we create).

#### command line options
We copy the test.py code into the notebook and we programatically generate the opt object which holds the command and default options for running the training. The model name needs to be specified(corresponds to the folder that holds the network weights inside checkpoints). We can also setup how many pictures from the testing folder will be used to perform the test. The dataset folder must contain the folders: trainA, trainB, testA, testB. There's an option to indicate which network we want to test, whether it's GA (from dataset A to B ) or GB (from dataset B to A )

### Training and testing a model


Our second experiment is to train the horse2zebra model by few epochs. The Colab notebook can be found here: https://colab.research.google.com/drive/1K4meKjG7dXwsBd2bYaNbB5ORt540wNA-

We download the dataset as explained before, but we remove the checkpoints folder.
We copy the train.py code into the notebook and setup an object opt that will hold all the command line options. We setup a 2 epochs trainig, with saving every 1 epoch, showing results every 250 samples trained with an image size of 128x128 seting up the resize and crop option. We deactivate all calls to the view server when executing this training in Colab. The training takes aroun 1700seconds per epoch.

Taking a look in the code, we observe that the networks used are 9 block Resnet for generative networks and PatchGAN for Discriminative networks.


### Caricatures datasets

Our third experiment is to try to train a CycleGan to generate images from a human faces dataset to a cariacture face dataset.  The Colab notebook can be found at: https://colab.research.google.com/drive/13sIRIdV9skUDuTEMo4d8QcA3-3YG_R4_
Also in the branch caricatures_local, the dlain_train.py and dlai_test.py preform the same task.

What datasets are available? we start with the celebrity faces dataset and a caricature dataset from Kaggle https://www.kaggle.com/ranjeetapegu/caricature-image. Both datasets are not of the same size, so we extract 10000 celebrity face pictures and distribute 5000 to each trainA and testA folder. Then we distribute both train and test pictures from caricatures dataset to trainB and testB. We observe that the caricatures dataset in use is not very homogeneous, has many caricatures showing parts of the body (not only the face) and that is limited to 8 celebrities only,(with many different caricatures for each celebrity). 

Did we setup any preprocessing? We set the training scrip to resize the images to 128x128, and to use 4 or 8 images per batch (depending on 4GB gpu or 8GB GPU). We try to  increment batchsize while GPU memory does not break.


We suggest to try the following improvements:

1. adapt network to 64x64 to be able to train faster. We guess the networks are not suitable for 64x64 images since the initial results where much worse than with 128x128 image sizes.

2. black&white another suggestion is to convert the images to black&white to get the networks to focus on contours an forms and forget about colors. We estimate this would have a better result visually.

3. transfer learning (fine-tuning): download a pretrained model (any could do work, but we suggest one that works with datasets that are close to hours). We would retrain again the whole model for a few epochs and see the results.

4. transfer learning (retraining last layers): download a pretrained model also but instead of training all of it, just update the weights on some of the last layers and freeze the rest. We guess that for this approach, we should update weights at least in all of the 4 networks (GA,DA,GB,DB) or even wors, since discriminator network must be very good to make generator network improve, we would maybe be forced to retrain all discriminator weights and just a few layers of the generator ones.


### Mini-CYCLEGAN
for a better understanding of the cyclegan we decided to implement a tiny/basic
cycleGAN and compare it with its homologue GAN.
For this we decided to work with one of the simplest and widely extended
benchmark dataset: the MNIST. So, our purpose is to generate MNIST numbers 
using a GAN and a cycle GAN.
We implement it ni pytorch and using CPU and GPU computational power.
We development has been carried out in google cloud plattform.


